{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c48fb205-9dca-47ec-8b2b-dfe71d7733c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import os\n",
    "from pathlib import Path\n",
    "import sys\n",
    " \n",
    "if sys.argv:\n",
    "    sys.path.insert(0, str(pathlib.Path(os.path.dirname(os.path.abspath(\"\"))).resolve()))\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fe2dc966-49e8-4a40-b9c3-0e6fd6f35a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models.IModelUplift import IModelUplift"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67ce0bf7-cd83-4d7d-acb1-b449f6d068f4",
   "metadata": {},
   "source": [
    "Descnn.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "961f0d5a-5a55-456f-bb3e-7d70ec40f0ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        stdv = 1 / math.sqrt(m.weight.size(1))\n",
    "        torch.nn.init.normal_(m.weight, mean=0.0, std=stdv)\n",
    "        # torch.nn.init.xavier_uniform_(m.weight)\n",
    "        m.bias.data.fill_(0)\n",
    "\n",
    "def safe_sqrt(x):\n",
    "    ''' Numerically safe version of Pytoch sqrt '''\n",
    "    return torch.sqrt(torch.clip(x, 1e-9, 1e+9))\n",
    "\n",
    "class ShareNetwork(nn.Module):\n",
    "    def __init__(self, input_dim, share_dim, base_dim, cfg, device):\n",
    "        super(ShareNetwork, self).__init__()\n",
    "        if cfg.get('BatchNorm1d', 'false') == 'true':\n",
    "            self.DNN = nn.Sequential(\n",
    "                nn.BatchNorm1d(input_dim),\n",
    "                nn.Linear(input_dim, share_dim),\n",
    "                nn.ELU(),\n",
    "                nn.Dropout(p=cfg.get('do_rate', 0.2)),\n",
    "                nn.Linear(share_dim, share_dim),\n",
    "                nn.ELU(),\n",
    "                nn.Dropout(p=cfg.get('do_rate', 0.2)),\n",
    "                nn.Linear(share_dim, base_dim),\n",
    "                nn.ELU(),\n",
    "                nn.Dropout(p=cfg.get('do_rate', 0.2))\n",
    "            )\n",
    "        else:\n",
    "            self.DNN = nn.Sequential(\n",
    "                nn.Linear(input_dim, share_dim),\n",
    "                nn.ELU(),\n",
    "                nn.Dropout(p=cfg.get('do_rate', 0.2)),\n",
    "                nn.Linear(share_dim, share_dim),\n",
    "                nn.ELU(),\n",
    "                nn.Dropout(p=cfg.get('do_rate', 0.2)),\n",
    "                nn.Linear(share_dim, base_dim),\n",
    "                nn.ELU(),\n",
    "            )\n",
    "\n",
    "        self.DNN.apply(init_weights)\n",
    "        self.cfg = cfg\n",
    "        self.device = device\n",
    "        self.to(device)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.to(self.device)\n",
    "        h_rep = self.DNN(x)\n",
    "        if self.cfg.get('normalization', 'none') == \"divide\":\n",
    "            h_rep_norm = h_rep / safe_sqrt(torch.sum(torch.square(h_rep), dim=1, keepdim=True))\n",
    "        else:\n",
    "            h_rep_norm = 1.0 * h_rep\n",
    "        return h_rep_norm\n",
    "\n",
    "\n",
    "class BaseModel(nn.Module):\n",
    "    def __init__(self, base_dim, cfg):\n",
    "        super(BaseModel, self).__init__()\n",
    "        self.DNN = nn.Sequential(\n",
    "            nn.Linear(base_dim, base_dim),\n",
    "            nn.ELU(),\n",
    "            nn.Dropout(p=cfg.get('do_rate', 0.2)),\n",
    "            nn.Linear(base_dim, base_dim),\n",
    "            nn.ELU(),\n",
    "            nn.Dropout(p=cfg.get('do_rate', 0.2)),\n",
    "            nn.Linear(base_dim, base_dim),\n",
    "            nn.ELU(),\n",
    "            nn.Dropout(p=cfg.get('do_rate', 0.2))\n",
    "        )\n",
    "        self.DNN.apply(init_weights)\n",
    "\n",
    "    def forward(self, x):\n",
    "        logits = self.DNN(x)\n",
    "        return logits\n",
    "\n",
    "\n",
    "class PrpsyNetwork(nn.Module):\n",
    "    \"\"\"propensity network\"\"\"\n",
    "    def __init__(self, base_dim, cfg):\n",
    "        super(PrpsyNetwork, self).__init__()\n",
    "        self.baseModel = BaseModel(base_dim, cfg)\n",
    "        self.logitLayer = nn.Linear(base_dim, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.logitLayer.apply(init_weights)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        inputs = self.baseModel(inputs)\n",
    "        p = self.logitLayer(inputs)\n",
    "        return p\n",
    "\n",
    "\n",
    "class Mu0Network(nn.Module):\n",
    "    def __init__(self, base_dim, cfg):\n",
    "        super(Mu0Network, self).__init__()\n",
    "        self.baseModel = BaseModel(base_dim, cfg)\n",
    "        self.logitLayer = nn.Linear(base_dim, 1)\n",
    "        self.logitLayer.apply(init_weights)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        inputs = self.baseModel(inputs)\n",
    "        p = self.logitLayer(inputs)\n",
    "        return p\n",
    "\n",
    "\n",
    "class Mu1Network(nn.Module):\n",
    "    def __init__(self, base_dim, cfg):\n",
    "        super(Mu1Network, self).__init__()\n",
    "        self.baseModel = BaseModel(base_dim, cfg)\n",
    "        self.logitLayer = nn.Linear(base_dim, 1)\n",
    "        self.logitLayer.apply(init_weights)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        inputs = self.baseModel(inputs)\n",
    "        p = self.logitLayer(inputs)\n",
    "        return p\n",
    "\n",
    "\n",
    "class TauNetwork(nn.Module):\n",
    "    \"\"\"pseudo tau network\"\"\"\n",
    "    def __init__(self, base_dim, cfg):\n",
    "        super(TauNetwork, self).__init__()\n",
    "        self.baseModel = BaseModel(base_dim, cfg)\n",
    "        self.logitLayer = nn.Linear(base_dim, 1)\n",
    "        self.logitLayer.apply(init_weights)\n",
    "        self.tanh = nn.Tanh()\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        inputs = self.baseModel(inputs)\n",
    "        tau_logit = self.logitLayer(inputs)\n",
    "        return tau_logit\n",
    "\n",
    "\n",
    "class DESCN(nn.Module):\n",
    "    \"\"\"DESCN (Deep End-to-end Stochastic Causal Network)\"\"\"\n",
    "    def __init__(self, input_dim, share_dim, base_dim, do_rate, device, batch_norm=False, normalization=\"none\"):\n",
    "        super(DESCN, self).__init__()\n",
    "        # Конфигурация модели\n",
    "        cfg = {\n",
    "            'do_rate': do_rate,\n",
    "            'BatchNorm1d': 'true' if batch_norm else 'false',\n",
    "            'normalization': normalization\n",
    "        }\n",
    "        \n",
    "        # Компоненты модели\n",
    "        self.shareNetwork = ShareNetwork(input_dim, share_dim, base_dim, cfg, device)\n",
    "        self.prpsy_network = PrpsyNetwork(base_dim, cfg)\n",
    "        self.mu1_network = Mu1Network(base_dim, cfg)\n",
    "        self.mu0_network = Mu0Network(base_dim, cfg)\n",
    "        self.tau_network = TauNetwork(base_dim, cfg)\n",
    "        \n",
    "        self.cfg = cfg\n",
    "        self.device = device\n",
    "        self.to(device)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        shared_h = self.shareNetwork(inputs)\n",
    "\n",
    "        # propensity output_logit\n",
    "        p_prpsy_logit = self.prpsy_network(shared_h)\n",
    "        p_prpsy = torch.clip(torch.sigmoid(p_prpsy_logit), 0.001, 0.999)\n",
    "\n",
    "        # logit for mu1, mu0\n",
    "        mu1_logit = self.mu1_network(shared_h)\n",
    "        mu0_logit = self.mu0_network(shared_h)\n",
    "\n",
    "        # pseudo tau\n",
    "        tau_logit = self.tau_network(shared_h)\n",
    "\n",
    "        p_mu1 = sigmod2(mu1_logit)\n",
    "        p_mu0 = sigmod2(mu0_logit)\n",
    "        p_h1 = p_mu1  # Refer to the naming in TARnet/CFR\n",
    "        p_h0 = p_mu0  # Refer to the naming in TARnet/CFR\n",
    "\n",
    "        # entire space\n",
    "        p_estr = torch.mul(p_prpsy, p_h1)\n",
    "        p_i_prpsy = 1 - p_prpsy\n",
    "        p_escr = torch.mul(p_i_prpsy, p_h0)\n",
    "        \n",
    "        # Рассчитываем аплифт (эффект воздействия)\n",
    "        uplift = mu1_logit - mu0_logit\n",
    "\n",
    "        return {\n",
    "            'p_prpsy_logit': p_prpsy_logit,\n",
    "            'p_estr': p_estr,\n",
    "            'p_escr': p_escr,\n",
    "            'tau_logit': tau_logit,\n",
    "            'mu1_logit': mu1_logit,\n",
    "            'mu0_logit': mu0_logit,\n",
    "            'p_prpsy': p_prpsy,\n",
    "            'p_mu1': p_mu1,\n",
    "            'p_mu0': p_mu0,\n",
    "            'p_h1': p_h1,\n",
    "            'p_h0': p_h0,\n",
    "            'shared_h': shared_h,\n",
    "            'uplift': uplift,\n",
    "            'y1': mu1_logit,\n",
    "            'y0': mu0_logit\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1380b8c5-a3fd-4a33-b23f-c1780dd9b453",
   "metadata": {},
   "source": [
    "### INNUpliftModeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3ee75374-2566-4576-9c9a-7311a6b600ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.metric import get_auuc_v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9323e8eb-7360-42c1-a086-2034df3edd6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from abc import ABC, abstractmethod\n",
    "from typing import Dict, List, Union, Optional, Tuple\n",
    "from torch.utils.data import DataLoader\n",
    "from src.models.IModelUplift import IModelUplift\n",
    "from src.datasets import TorchDataset\n",
    "\n",
    "class INNUpliftModeling(IModelUplift):\n",
    "    \"\"\"\n",
    "    Родительский класс для реализации нейросетевых моделей аплифт-моделирования.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, config_json=None, from_load=False, path=None):\n",
    "        \"\"\"\n",
    "        Инициализация объекта модели.\n",
    "        \n",
    "        Args:\n",
    "            config_json: строка с JSON-конфигурацией модели\n",
    "            from_load: флаг, указывающий, что модель загружается из файла\n",
    "            path: путь для загрузки модели\n",
    "        \"\"\"\n",
    "        super().__init__(config_json, from_load, path)\n",
    "\n",
    "        if from_load == False:\n",
    "            if config_json is None:\n",
    "                raise ValueError(f\"No config while contstructing model.\")\n",
    "\n",
    "            if isinstance(config_json, str):\n",
    "                self.config = json.loads(config_json)\n",
    "            else:\n",
    "                self.config = config_json\n",
    "            self.model = None\n",
    "            self.device = torch.device('cuda' if torch.cuda.is_available() and self.config.get('use_gpu', True) else 'cpu')\n",
    "            self._initialize_model()\n",
    "            self._setup_optimizer_and_scheduler()\n",
    "        else:\n",
    "            if path is None:\n",
    "                raise ValueError(f\"No config or model paths while contstructing model.\")\n",
    "            self.load(path)\n",
    "    \n",
    "    def _initialize_model(self):\n",
    "        pass\n",
    "    \n",
    "    def _setup_optimizer_and_scheduler(self):\n",
    "        \"\"\"\n",
    "        Инициализация оптимизатора и планировщика лр.\n",
    "        \"\"\"\n",
    "        optimizer_config = self.config.get('optimizer', {})\n",
    "        optimizer_name = optimizer_config.get('name', 'Adam')\n",
    "        lr = optimizer_config.get('lr', 0.001)\n",
    "        weight_decay = optimizer_config.get('weight_decay', 0.0)\n",
    "        \n",
    "        if optimizer_name == 'Adam':\n",
    "            self.optimizer = torch.optim.Adam(\n",
    "                self.model.parameters(), \n",
    "                lr=lr, \n",
    "                weight_decay=weight_decay\n",
    "            )\n",
    "        elif optimizer_name == 'SGD':\n",
    "            momentum = optimizer_config.get('momentum', 0.9)\n",
    "            self.optimizer = torch.optim.SGD(\n",
    "                self.model.parameters(), \n",
    "                lr=lr, \n",
    "                momentum=momentum, \n",
    "                weight_decay=weight_decay\n",
    "            )\n",
    "        elif optimizer_name == 'AdamW':\n",
    "            self.optimizer = torch.optim.AdamW(\n",
    "                self.model.parameters(), \n",
    "                lr=lr, \n",
    "                weight_decay=weight_decay\n",
    "            )\n",
    "        else:\n",
    "                self.optimizer = torch.optim.AdamW(\n",
    "                self.model.parameters(), \n",
    "                lr=lr, \n",
    "                weight_decay=weight_decay\n",
    "            )\n",
    "        \n",
    "        scheduler_config = self.config.get('scheduler', {})\n",
    "        scheduler_name = scheduler_config.get('name')\n",
    "        \n",
    "        if scheduler_name == 'ReduceLROnPlateau':\n",
    "            self.scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "                self.optimizer,\n",
    "                mode=scheduler_config.get('mode', 'min'),\n",
    "                factor=scheduler_config.get('factor', 0.1),\n",
    "                patience=scheduler_config.get('patience', 10),\n",
    "                verbose=scheduler_config.get('verbose', True)\n",
    "            )\n",
    "        elif scheduler_name == 'CosineAnnealingLR':\n",
    "            self.scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "                self.optimizer,\n",
    "                T_max=scheduler_config.get('T_max', 100),\n",
    "                eta_min=scheduler_config.get('eta_min', 0)\n",
    "            )\n",
    "        elif scheduler_name is None:\n",
    "            self.scheduler = None\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported scheduler: {scheduler_name}\")\n",
    "    \n",
    "    def _get_data_loader(self, X: TorchDataset, batch_size=None, shuffle=False):\n",
    "        \"\"\"\n",
    "        Создание дата лоадера.\n",
    "        \"\"\"\n",
    "        if batch_size is None:\n",
    "            batch_size = self.config.get('batch_size', 32)\n",
    "            \n",
    "        return DataLoader(\n",
    "            X, \n",
    "            batch_size=batch_size, \n",
    "            shuffle=shuffle, \n",
    "            num_workers=self.config.get('num_workers', 0)\n",
    "        )\n",
    "    \n",
    "    \n",
    "    def _compute_loss(self, outputs, outcome, treatment):\n",
    "        \"\"\"\n",
    "        Вычисление лосса\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def fit(self, X_train: TorchDataset):\n",
    "        \"\"\"\n",
    "        Обучение модели с валидацией.            \n",
    "        История обучения (словарь с метриками по эпохам)\n",
    "        \"\"\"\n",
    "        train_size = int(0.8 * len(X_train))\n",
    "        val_size = len(X_train) - train_size\n",
    "        X_train, X_val = torch.utils.data.random_split(X_train, [train_size, val_size])\n",
    "        \n",
    "        self.model.train()\n",
    "        \n",
    "        epochs = self.config.get('epochs', 20)\n",
    "        batch_size = self.config.get('batch_size', 32)\n",
    "        early_stopping_patience = self.config.get('early_stopping_patience', 10)\n",
    "        accumulation_steps = self.config.get('gradient_accumulation_steps', 1)\n",
    "        effective_batch_size = batch_size * accumulation_steps\n",
    "        \n",
    "        train_loader = self._prepare_data_loader(X_train, batch_size, shuffle=True)\n",
    "        val_loader = self._prepare_data_loader(X_val, batch_size, shuffle=False)\n",
    "        \n",
    "        best_val_loss = float('inf')\n",
    "        best_val_auuc = float('-inf')\n",
    "        early_stopping_criterion = self.config.get('early_stopping_criterion', 'loss')  # 'loss' или 'auuc'\n",
    "        patience_counter = 0\n",
    "        \n",
    "        history = {\n",
    "            'epoch': [],\n",
    "            'train_loss': [],\n",
    "            'val_loss': [],\n",
    "            'val_auuc': [],\n",
    "            'learning_rate': []\n",
    "        }\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            epoch_loss = 0.0\n",
    "            num_batches = 0\n",
    "            \n",
    "            self.model.train()\n",
    "            \n",
    "            for batch in train_loader:\n",
    "                features, treatment, outcome = batch                \n",
    "                self.optimizer.zero_grad()\n",
    "                \n",
    "                outputs = self._forward_pass(features, treatment)\n",
    "                \n",
    "                loss = self._compute_loss(outputs, outcome, treatment) / accumulation_steps\n",
    "                loss.backward()\n",
    "                \n",
    "                epoch_loss += loss.item() * accumulation_steps\n",
    "                num_batches += 1\n",
    "                \n",
    "                if (batch_idx + 1) % accumulation_steps == 0 or (batch_idx + 1) == len(train_loader):\n",
    "                    self.optimizer.step()\n",
    "                    self.optimizer.zero_grad()\n",
    "                    \n",
    "                    if (batch_idx + 1) % max(1, len(train_loader) // 10) == 0:\n",
    "                        progress = (batch_idx + 1) / len(train_loader) * 100\n",
    "                        current_loss = epoch_loss / num_batches\n",
    "                        print(f\"Epoch {epoch+1}/{epochs} - {progress:.1f}% - Loss: {current_loss:.4f}\")\n",
    "             \n",
    "            avg_train_loss = epoch_loss / num_batches\n",
    "\n",
    "            # ---- validation ----\n",
    "            val_loss, val_auuc = self._evaluate(val_loader)\n",
    "            \n",
    "            if self.scheduler is not None:\n",
    "                if isinstance(self.scheduler, torch.optim.lr_scheduler.ReduceLROnPlateau):\n",
    "                    self.scheduler.step(val_loss)\n",
    "                else:\n",
    "                    self.scheduler.step()\n",
    "\n",
    "            \n",
    "            history['epoch'].append(epoch + 1)\n",
    "            history['train_loss'].append(avg_train_loss)\n",
    "            history['val_loss'].append(val_loss)\n",
    "            history['val_auuc'].append(val_auuc)\n",
    "            history['learning_rate'].append(self.optimizer.param_groups[0]['lr'])\n",
    "            \n",
    "            print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {avg_train_loss:.4f}, \"\n",
    "                  f\"Val Loss: {val_loss:.4f}, Val AUUC: {val_auuc:.4f}, \"\n",
    "                  f\"LR: {self.optimizer.param_groups[0]['lr']:.6f}\")\n",
    "            \n",
    "            if early_stopping_criterion == 'loss' and val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                patience_counter = 0\n",
    "                \n",
    "                best_model_state = {name: param.clone() for name, param in self.model.state_dict().items()}\n",
    "            elif early_stopping_criterion == 'auuc' and val_auuc > best_val_auuc:\n",
    "                best_val_auuc = val_auuc\n",
    "                patience_counter = 0\n",
    "                \n",
    "                best_model_state = {name: param.clone() for name, param in self.model.state_dict().items()}\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "                if patience_counter >= early_stopping_patience:\n",
    "                    print(f\"Early stopping triggered after {epoch+1} epochs\")\n",
    "                    \n",
    "                    self.model.load_state_dict(best_model_state)\n",
    "                    break\n",
    "        \n",
    "        return history\n",
    "\n",
    "    def _evaluate(self, data_loader):\n",
    "        \"\"\"\n",
    "        loss и AUUC на валидационном или тестовом наборе.\n",
    "        \"\"\"\n",
    "        self.model.eval()\n",
    "        \n",
    "        total_loss = 0.0\n",
    "        num_batches = 0\n",
    "        \n",
    "        all_uplift_scores = []\n",
    "        all_treatments = []\n",
    "        all_outcomes = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in data_loader:\n",
    "                features, treatment, outcome = batch\n",
    "            \n",
    "                outputs = self._forward_pass(features, treatment)                \n",
    "                loss = self._compute_loss(outputs, outcome, treatment)\n",
    "                total_loss += loss.item()\n",
    "                num_batches += 1\n",
    "                \n",
    "                # Сбор данных для расчета AUUC\n",
    "                all_uplift_scores.append(uplift_scores.cpu())\n",
    "                all_treatments.append(treatment.cpu())\n",
    "                all_outcomes.append(outcome.cpu())\n",
    "        \n",
    "        avg_loss = total_loss / num_batches\n",
    "        auuc = float('nan')\n",
    "    \n",
    "        uplift_scores = torch.cat(all_uplift_scores, dim=0).numpy().flatten()\n",
    "        treatments = torch.cat(all_treatments, dim=0).numpy().flatten()\n",
    "        outcomes = torch.cat(all_outcomes, dim=0).numpy().flatten()                \n",
    "        auuc = get_auuc_v2(uplift_scores, treatments, outcomes)\n",
    "        \n",
    "        return avg_loss, auuc\n",
    "        \n",
    "    def predict(self, X: TorchDataset):\n",
    "        \"\"\"\n",
    "        Предсказание вероятностей и аплифт-скоров.\n",
    "        \"\"\"\n",
    "        self.model.eval()\n",
    "        batch_size = self.config.get('inference_batch_size', 32)\n",
    "        data_loader = self._prepare_data_loader(X, batch_size, shuffle=False)\n",
    "        \n",
    "        y0_list, y1_list, uplift_list = [], [], []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in data_loader:\n",
    "                features, treatment, _ = batch\n",
    "                \n",
    "                outputs = self.model(features)\n",
    "                \n",
    "                y0_list.append(outputs['y0'].cpu())\n",
    "                y1_list.append(outputs['y1'].cpu())\n",
    "                uplift_list.append(outputs['uplift'].cpu())\n",
    "        y0 = torch.cat(y0_list, dim=0).numpy()\n",
    "        y1 = torch.cat(y1_list, dim=0).numpy()\n",
    "        uplift = torch.cat(uplift_list, dim=0).numpy()\n",
    "        \n",
    "        return {\n",
    "            'y0': y0,\n",
    "            'y1': y1,\n",
    "            'uplift': uplift\n",
    "        }\n",
    "    \n",
    "    def predict_light(self, X: TorchDataset):\n",
    "        \"\"\"\n",
    "        Легкая версия предсказания (без возврата значений).\n",
    "        \"\"\"\n",
    "        # self.model.eval()\n",
    "        \n",
    "        # batch_size = self.config.get('inference_batch_size', 64)\n",
    "        # data_loader = self._prepare_data_loader(X, batch_size, shuffle=False)\n",
    "        \n",
    "        # with torch.no_grad():\n",
    "        #     for batch in data_loader:\n",
    "        #         features, treatment, _ = batch\n",
    "        #         _ = self.model(features)\n",
    "        pass\n",
    "    \n",
    "    def save(self, path):\n",
    "        \"\"\"\n",
    "        Сохранение модели в файл.\n",
    "        \"\"\"\n",
    "        os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "        \n",
    "        # Подготовка данных для сохранения\n",
    "        save_data = {\n",
    "            'model_state_dict': self.model.state_dict(),\n",
    "            'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "            'config': self.config\n",
    "        }\n",
    "        \n",
    "        if self.scheduler is not None:\n",
    "            save_data['scheduler_state_dict'] = self.scheduler.state_dict()\n",
    "        \n",
    "        torch.save(save_data, path)\n",
    "    \n",
    "    def load(self, path):\n",
    "        \"\"\"\n",
    "        Загрузка модели из файла.\n",
    "        \"\"\"\n",
    "        if not os.path.exists(path):\n",
    "            raise FileNotFoundError(f\"Model file not found: {path}\")\n",
    "            \n",
    "        checkpoint = torch.load(path, map_location=lambda storage, loc: storage)\n",
    "        self.config = checkpoint['config']\n",
    "        \n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() and \n",
    "                                  self.config.get('use_gpu', True) else 'cpu')\n",
    "        \n",
    "        self._initialize_model()\n",
    "        self.model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        \n",
    "        self._setup_optimizer_and_scheduler()\n",
    "        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        \n",
    "        if 'scheduler_state_dict' in checkpoint and self.scheduler is not None:\n",
    "            self.scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "    \n",
    "    def measure_inference_time(self, data, batch_size, max_size=None):\n",
    "        \"\"\"\n",
    "        Измерение среднего времени инференса модели на данных.\n",
    "        \"\"\"\n",
    "        max_size = 5000\n",
    "        batch_size=32\n",
    "        indices = torch.randperm(len(data))[:max_size]\n",
    "        subset_data = torch.utils.data.Subset(data, indices)\n",
    "        data_loader = self._prepare_data_loader(subset_data, batch_size, shuffle=False)\n",
    "        \n",
    "        self.model.eval()\n",
    "        \n",
    "        # Измерение времени\n",
    "        inference_times = []\n",
    "    \n",
    "        cur_size = 0\n",
    "        for batch in data_loader:\n",
    "            start_time = time.time()\n",
    "            predictions = self.forward(batch)\n",
    "            end_time = time.time() \n",
    "            \n",
    "            inference_times.append((end_time - start_time) * 1000 / batch_size)\n",
    "    \n",
    "            cur_size += batch_size\n",
    "            if cur_size >= max_size:\n",
    "                break\n",
    "    \n",
    "        mean_inference_time = np.mean(inference_times)\n",
    "        return mean_inference_time\n",
    "    \n",
    "    @staticmethod\n",
    "    def generate_config(count, **params):\n",
    "        \"\"\"\n",
    "        Генерация набора конфигураций для различных моделей.\n",
    "        \n",
    "        Args:\n",
    "            count: количество конфигураций для генерации\n",
    "            **params: дополнительные параметры и диапазоны для конфигураций\n",
    "            \n",
    "        Returns:\n",
    "            Список словарей с конфигурациями\n",
    "        \"\"\"\n",
    "        configs = []\n",
    "        \n",
    "        # Базовая конфигурация\n",
    "        base_config = {\n",
    "            'batch_size': 64,\n",
    "            'epochs': 100,\n",
    "            'early_stopping_patience': 10,\n",
    "            'optimizer': {\n",
    "                'name': 'Adam',\n",
    "                'lr': 0.001,\n",
    "                'weight_decay': 0.0001\n",
    "            },\n",
    "            'scheduler': {\n",
    "                'name': 'ReduceLROnPlateau',\n",
    "                'patience': 5,\n",
    "                'factor': 0.5\n",
    "            },\n",
    "            'use_gpu': True,\n",
    "            'num_workers': 2,\n",
    "            'inference_batch_size': 128\n",
    "        }\n",
    "        \n",
    "        # Объединение базовой конфигурации с переданными параметрами\n",
    "        for key, value in params.items():\n",
    "            if isinstance(value, list):\n",
    "                # Если передан список значений, будем перебирать их\n",
    "                base_config[key] = value[0]  # Используем первое значение как базовое\n",
    "            else:\n",
    "                base_config[key] = value\n",
    "        \n",
    "        # Генерация вариаций конфигураций\n",
    "        for i in range(count):\n",
    "            config = base_config.copy()\n",
    "            \n",
    "            # Модификация конфигурации на основе переданных параметров\n",
    "            for key, value in params.items():\n",
    "                if isinstance(value, list):\n",
    "                    # Выбираем случайное значение из списка\n",
    "                    config[key] = np.random.choice(value)\n",
    "                elif isinstance(value, tuple) and len(value) == 2:\n",
    "                    # Если передан диапазон (min, max), генерируем случайное значение\n",
    "                    min_val, max_val = value\n",
    "                    if isinstance(min_val, int) and isinstance(max_val, int):\n",
    "                        config[key] = np.random.randint(min_val, max_val + 1)\n",
    "                    else:\n",
    "                        config[key] = np.random.uniform(min_val, max_val)\n",
    "            \n",
    "            configs.append(config)\n",
    "        \n",
    "        return configs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "355256b6-8c38-447f-9439-804d5d1a830d",
   "metadata": {},
   "source": [
    "DESCNUplift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "ca78e123-879e-4ad7-84cd-1c52a1a9693a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DESCNUpliftModel(INNUpliftModeling):\n",
    "    \"\"\"\n",
    "    Реализация модели DESCN для аплифт-моделирования.\n",
    "    \"\"\"\n",
    "    \n",
    "    def _initialize_model(self):\n",
    "        \"\"\"\n",
    "        Инициализация архитектуры модели DESCN.\n",
    "        \"\"\"\n",
    "        input_dim = self.config.get('input_dim')\n",
    "        share_dim = self.config.get('share_dim', 128)\n",
    "        base_dim = self.config.get('base_dim', 64)\n",
    "        do_rate = self.config.get('do_rate', 0.2)\n",
    "        batch_norm = self.config.get('batch_norm', False)\n",
    "        normalization = self.config.get('normalization', 'none')\n",
    "        \n",
    "        # Проверка наличия обязательных параметров\n",
    "        if input_dim is None:\n",
    "            raise ValueError(\"input_dim must be specified in the config\")\n",
    "        \n",
    "        # Инициализация модели\n",
    "        self.model = DESCN(\n",
    "            input_dim=input_dim,\n",
    "            share_dim=share_dim,\n",
    "            base_dim=base_dim,\n",
    "            do_rate=do_rate,\n",
    "            device=self.device,\n",
    "            batch_norm=batch_norm,\n",
    "            normalization=normalization\n",
    "        )\n",
    "    \n",
    "    def _compute_loss(self, outputs, outcome, treatment):\n",
    "        \"\"\"\n",
    "        Вычисление функции потерь для DESCN.\n",
    "        \n",
    "        Args:\n",
    "            outputs: выход модели\n",
    "            outcome: целевая переменная\n",
    "            treatment: индикатор воздействия\n",
    "            \n",
    "        Returns:\n",
    "            Значение функции потерь\n",
    "        \"\"\"\n",
    "        # Извлечение необходимых выходов модели\n",
    "        mu1_logit = outputs['mu1_logit']\n",
    "        mu0_logit = outputs['mu0_logit']\n",
    "        p_prpsy_logit = outputs['p_prpsy_logit']\n",
    "        \n",
    "        # Веса для разных компонентов потери\n",
    "        factual_loss_weight = self.config.get('factual_loss_weight', 1.0)\n",
    "        propensity_loss_weight = self.config.get('propensity_loss_weight', 0.1)\n",
    "        tau_loss_weight = self.config.get('tau_loss_weight', 0.1)\n",
    "        \n",
    "        # Формируем маски для групп воздействия и контроля\n",
    "        treatment_mask = (treatment == 1).float().unsqueeze(1)\n",
    "        control_mask = (treatment == 0).float().unsqueeze(1)\n",
    "        \n",
    "        # Фактическая потеря - MSE для фактических наблюдений\n",
    "        y_pred = treatment_mask * mu1_logit + control_mask * mu0_logit\n",
    "        factual_loss = F.mse_loss(y_pred, outcome.unsqueeze(1))\n",
    "        \n",
    "        # Потеря для предсказания вероятности назначения воздействия\n",
    "        propensity_loss = F.binary_cross_entropy_with_logits(\n",
    "            p_prpsy_logit.squeeze(), \n",
    "            treatment\n",
    "        )\n",
    "        \n",
    "        # Потеря для предсказания эффекта воздействия (если известно)\n",
    "        if self.config.get('use_tau_loss', False) and hasattr(self, 'tau_true'):\n",
    "            tau_loss = F.mse_loss(outputs['tau_logit'], self.tau_true)\n",
    "        else:\n",
    "            tau_loss = torch.tensor(0.0, device=self.device)\n",
    "        \n",
    "        # Общая потеря\n",
    "        total_loss = (\n",
    "            factual_loss_weight * factual_loss + \n",
    "            propensity_loss_weight * propensity_loss + \n",
    "            tau_loss_weight * tau_loss\n",
    "        )\n",
    "        \n",
    "        return total_loss\n",
    "    \n",
    "    def _process_prediction_outputs(self, outputs):\n",
    "        \"\"\"\n",
    "        Обработка выходов модели для предсказания.\n",
    "        \n",
    "        Args:\n",
    "            outputs: выходы модели\n",
    "            \n",
    "        Returns:\n",
    "            Словарь с предсказанными значениями\n",
    "        \"\"\"\n",
    "        # Выделяем и преобразуем нужные для предсказания поля\n",
    "        return {\n",
    "            'y0': outputs['mu0_logit'],\n",
    "            'y1': outputs['mu1_logit'],\n",
    "            'uplift': outputs['uplift'],\n",
    "            'propensity': outputs['p_prpsy']\n",
    "        }\n",
    "    \n",
    "    @staticmethod\n",
    "    def generate_config(count, **params):\n",
    "        \"\"\"\n",
    "        Генерация конфигураций для DESCN модели.\n",
    "        \n",
    "        Args:\n",
    "            count: количество конфигураций\n",
    "            **params: дополнительные параметры\n",
    "            \n",
    "        Returns:\n",
    "            Список конфигураций\n",
    "        \"\"\"\n",
    "        # Базовые параметры для DESCN\n",
    "        descn_params = {\n",
    "            'input_dim': 100,             # Должно быть задано в соответствии с данными\n",
    "            'share_dim': [256, 256], # Варианты размерности общих слоев\n",
    "            'base_dim': [256],   # Варианты размерности базовых слоев\n",
    "            'do_rate': [0.1, 0.2, 0.3],  # Варианты dropout\n",
    "            'batch_norm': [True, False], # Использование BatchNorm\n",
    "            'normalization': ['none', 'divide'], # Тип нормализации\n",
    "            'factual_loss_weight': [0.8, 1.0, 1.2], # Вес фактической потери\n",
    "            'propensity_loss_weight': [0.05, 0.1, 0.2], # Вес потери пропенсити\n",
    "            'tau_loss_weight': [0.05, 0.1, 0.2],    # Вес потери tau (если применимо)\n",
    "            'gradient_accumulation_steps' : 2\n",
    "        }\n",
    "        \n",
    "        # Объединение с переданными параметрами\n",
    "        for key, value in params.items():\n",
    "            descn_params[key] = value\n",
    "        \n",
    "        # Генерация конфигураций с использованием базового метода\n",
    "        return INNUpliftModeling.generate_config(count, **descn_params)\n",
    "\n",
    "    def num_params(self):\n",
    "        return sum([p.numel() for p in self.model.parameters() if p.requires_grad])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "f542ad56-5b0f-4d1d-9161-9c64d3ba3447",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'batch_size': 64, 'epochs': 100, 'early_stopping_patience': 10, 'optimizer': {'name': 'Adam', 'lr': 0.001, 'weight_decay': 0.0001}, 'scheduler': {'name': 'ReduceLROnPlateau', 'patience': 5, 'factor': 0.5}, 'use_gpu': True, 'num_workers': 2, 'inference_batch_size': 128, 'input_dim': 100, 'share_dim': 256, 'base_dim': 256, 'do_rate': 0.2, 'batch_norm': False, 'normalization': 'none', 'factual_loss_weight': 0.8, 'propensity_loss_weight': 0.1, 'tau_loss_weight': 0.1}\n",
      "{'batch_size': 64, 'epochs': 100, 'early_stopping_patience': 10, 'optimizer': {'name': 'Adam', 'lr': 0.001, 'weight_decay': 0.0001}, 'scheduler': {'name': 'ReduceLROnPlateau', 'patience': 5, 'factor': 0.5}, 'use_gpu': True, 'num_workers': 2, 'inference_batch_size': 128, 'input_dim': 100, 'share_dim': 256, 'base_dim': 256, 'do_rate': 0.1, 'batch_norm': False, 'normalization': 'none', 'factual_loss_weight': 0.8, 'propensity_loss_weight': 0.2, 'tau_loss_weight': 0.05}\n",
      "{'batch_size': 64, 'epochs': 100, 'early_stopping_patience': 10, 'optimizer': {'name': 'Adam', 'lr': 0.001, 'weight_decay': 0.0001}, 'scheduler': {'name': 'ReduceLROnPlateau', 'patience': 5, 'factor': 0.5}, 'use_gpu': True, 'num_workers': 2, 'inference_batch_size': 128, 'input_dim': 100, 'share_dim': 256, 'base_dim': 256, 'do_rate': 0.2, 'batch_norm': True, 'normalization': 'divide', 'factual_loss_weight': 1.0, 'propensity_loss_weight': 0.2, 'tau_loss_weight': 0.1}\n"
     ]
    }
   ],
   "source": [
    "kek = DESCNUpliftModel.generate_config(count=3)\n",
    "for a in kek:\n",
    "    print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "489e7e24-4f44-4dc3-917b-c50440ced4ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to import duecredit due to No module named 'duecredit'\n"
     ]
    }
   ],
   "source": [
    "from src.utils import get_paths_train_test, train_test_model\n",
    "from src.factory import SModelFactory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb47b2da-4851-496f-b1aa-6e8d503fe022",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DESCNUpliftModelFactory(IFactory):\n",
    "    @staticmethod\n",
    "    def create(config_json, train_path, test_path):\n",
    "        model = DESCNUpliftModel(config_json)\n",
    "        train = NumpyDataset(train_path)\n",
    "        test = NumpyDataset(test_path)\n",
    "        return model, train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "8b551616-863a-4970-9c9c-e30a7ef023ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ogrobertino/test_env/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model = DESCNUpliftModel(kek[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "064c277d-bde9-4b4a-a56f-e66e99634ceb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "947972"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.num_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "3202ce86-a6c7-4b73-970c-050332fb60af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "930564"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([p.numel() for p in model.model.parameters() if p.requires_grad])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b0baa2ba-7f95-4898-8445-93c199898622",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object Module.parameters at 0x7f2b6df1f3e0>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.model.parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "434b766d-271d-4694-b4b7-8d5083bac29b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'batch_size': 64,\n",
       " 'epochs': 100,\n",
       " 'early_stopping_patience': 10,\n",
       " 'optimizer': {'name': 'Adam', 'lr': 0.001, 'weight_decay': 0.0001},\n",
       " 'scheduler': {'name': 'ReduceLROnPlateau', 'patience': 5, 'factor': 0.5},\n",
       " 'use_gpu': True,\n",
       " 'num_workers': 2,\n",
       " 'inference_batch_size': 128,\n",
       " 'input_dim': 32,\n",
       " 'share_dim': 128,\n",
       " 'base_dim': 64,\n",
       " 'do_rate': 0.2,\n",
       " 'batch_norm': True,\n",
       " 'normalization': 'divide',\n",
       " 'factual_loss_weight': 1.2,\n",
       " 'propensity_loss_weight': 0.1,\n",
       " 'tau_loss_weight': 0.1}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f4230ea5-6f1a-44f4-a6e5-a22c5d196079",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DESCN(\n",
       "  (shareNetwork): ShareNetwork(\n",
       "    (DNN): Sequential(\n",
       "      (0): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (1): Linear(in_features=32, out_features=128, bias=True)\n",
       "      (2): ELU(alpha=1.0)\n",
       "      (3): Dropout(p=0.2, inplace=False)\n",
       "      (4): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (5): ELU(alpha=1.0)\n",
       "      (6): Dropout(p=0.2, inplace=False)\n",
       "      (7): Linear(in_features=128, out_features=64, bias=True)\n",
       "      (8): ELU(alpha=1.0)\n",
       "      (9): Dropout(p=0.2, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (prpsy_network): PrpsyNetwork(\n",
       "    (baseModel): BaseModel(\n",
       "      (DNN): Sequential(\n",
       "        (0): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (1): ELU(alpha=1.0)\n",
       "        (2): Dropout(p=0.2, inplace=False)\n",
       "        (3): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (4): ELU(alpha=1.0)\n",
       "        (5): Dropout(p=0.2, inplace=False)\n",
       "        (6): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (7): ELU(alpha=1.0)\n",
       "        (8): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (logitLayer): Linear(in_features=64, out_features=1, bias=True)\n",
       "    (sigmoid): Sigmoid()\n",
       "  )\n",
       "  (mu1_network): Mu1Network(\n",
       "    (baseModel): BaseModel(\n",
       "      (DNN): Sequential(\n",
       "        (0): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (1): ELU(alpha=1.0)\n",
       "        (2): Dropout(p=0.2, inplace=False)\n",
       "        (3): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (4): ELU(alpha=1.0)\n",
       "        (5): Dropout(p=0.2, inplace=False)\n",
       "        (6): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (7): ELU(alpha=1.0)\n",
       "        (8): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (logitLayer): Linear(in_features=64, out_features=1, bias=True)\n",
       "    (sigmoid): Sigmoid()\n",
       "    (relu): ReLU()\n",
       "  )\n",
       "  (mu0_network): Mu0Network(\n",
       "    (baseModel): BaseModel(\n",
       "      (DNN): Sequential(\n",
       "        (0): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (1): ELU(alpha=1.0)\n",
       "        (2): Dropout(p=0.2, inplace=False)\n",
       "        (3): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (4): ELU(alpha=1.0)\n",
       "        (5): Dropout(p=0.2, inplace=False)\n",
       "        (6): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (7): ELU(alpha=1.0)\n",
       "        (8): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (logitLayer): Linear(in_features=64, out_features=1, bias=True)\n",
       "    (sigmoid): Sigmoid()\n",
       "    (relu): ReLU()\n",
       "  )\n",
       "  (tau_network): TauNetwork(\n",
       "    (baseModel): BaseModel(\n",
       "      (DNN): Sequential(\n",
       "        (0): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (1): ELU(alpha=1.0)\n",
       "        (2): Dropout(p=0.2, inplace=False)\n",
       "        (3): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (4): ELU(alpha=1.0)\n",
       "        (5): Dropout(p=0.2, inplace=False)\n",
       "        (6): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (7): ELU(alpha=1.0)\n",
       "        (8): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (logitLayer): Linear(in_features=64, out_features=1, bias=True)\n",
       "    (tanh): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "c79e161c-e0fc-43dd-978b-9ceefd2cb6f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "__main__.DESCNUpliftModel"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DESCNUpliftModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3148acaf-608d-4d57-84d9-98577a2a3b5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils import get_paths_train_test, train_test_model\n",
    "from src.factory import XModelFactory\n",
    "from src.configs_generation import generate_random_configs_xmodel\n",
    "from tqdm import tqdm\n",
    "configs = generate_random_configs_xmodel(parameters, count=1)\n",
    "ds_name = 'lazada'\n",
    "features_percent = 50\n",
    "factory = XModelFactory\n",
    "config = configs[0]\n",
    "batch_size=32\n",
    "max_size=100000\n",
    "train_path, test_path = get_paths_train_test(ds_name=ds_name, features_percent=features_percent)\n",
    "model, train, test = factory.create(config, train_path, test_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
