{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9b902205-cbb3-4ce7-9725-2223023b9b3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.93879991])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.normal(0, 1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "63758633-fe84-40c2-bee6-239993e645ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC для контрольной группы: 0.49\n",
      "AUC для тестовой группы (treatment): 0.63\n",
      "Пример uplift:\n",
      "[[0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from causalml.inference.tree import UpliftTreeClassifier\n",
    "from causalml.inference.meta import BaseTLearner\n",
    "from sklearn.metrics import classification_report, roc_auc_score\n",
    "import lightgbm as lgb\n",
    "\n",
    "# Синтетический набор данных (можно заменить на реальный)\n",
    "def generate_synthetic_data(n=5000):\n",
    "    np.random.seed(42)\n",
    "    X1 = np.random.normal(0, 1, n)\n",
    "    X2 = np.random.normal(0, 1, n)\n",
    "    X3 = np.random.normal(0, 1, n)\n",
    "\n",
    "    # Фичи\n",
    "    X = pd.DataFrame({\"X1\": X1, \"X2\": X2, \"X3\": X3})\n",
    "\n",
    "    # Третий признак влияет на вероятность записи лечения\n",
    "    treatment_effect = (X3 > 0).astype(int)\n",
    "    treatment = np.random.binomial(1, 0.5 + 0.2 * treatment_effect)\n",
    "\n",
    "    # Целевая метка\n",
    "    y = np.random.binomial(1, 0.3 + 0.4 * treatment * np.clip(X1 + X2, 0, 1))\n",
    "\n",
    "    return X, treatment, y\n",
    "\n",
    "# Генерация данных\n",
    "X, treatment, y = generate_synthetic_data()\n",
    "\n",
    "# Разделение тренировочного и тестового наборов данных\n",
    "X_train, X_test, treatment_train, treatment_test, y_train, y_test = train_test_split(\n",
    "    X, treatment, y, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "# Используем T-Learner\n",
    "t_learner = BaseTLearner(learner=RandomForestClassifier(random_state=42))\n",
    "\n",
    "# Обучение T-Learner\n",
    "t_learner.fit(X=X_train.values, treatment=treatment_train, y=y_train)\n",
    "\n",
    "tmodel = tlearner.BaseTClassifier(\n",
    "    # learner=lgb.LGBMClassifier(max_depth=4),\n",
    "    control_learner=lgb.LGBMClassifier(),\n",
    "    treatment_learner=lgb.LGBMClassifier(),\n",
    "    control_name=0\n",
    ")\n",
    "\n",
    "# # Прогноз uplift (оставим для примера)\n",
    "# uplift_pred = t_learner.predict(X_test.values)\n",
    "\n",
    "# # Оценка качества модели - бинарная классификация по группам\n",
    "# # Модель для контрольной группы (treatment=0)\n",
    "# model_c = RandomForestClassifier()\n",
    "# model_c.fit(X_train[treatment_train == 0], y_train[treatment_train == 0])\n",
    "\n",
    "# # Модель для тестовой группы (treatment=1)\n",
    "# model_t = RandomForestClassifier()\n",
    "# model_t.fit(X_train[treatment_train == 1], y_train[treatment_train == 1])\n",
    "\n",
    "# # Предсказания\n",
    "# y_pred_control = model_c.predict(X_test)\n",
    "# y_pred_treatment = model_t.predict(X_test)\n",
    "\n",
    "# # Оценка - например, через AUC для каждой группы\n",
    "# auc_control = roc_auc_score(y_test[treatment_test == 0], y_pred_control[treatment_test == 0])\n",
    "# auc_treatment = roc_auc_score(y_test[treatment_test == 1], y_pred_treatment[treatment_test == 1])\n",
    "\n",
    "# print(f\"AUC для контрольной группы: {auc_control:.2f}\")\n",
    "# print(f\"AUC для тестовой группы (treatment): {auc_treatment:.2f}\")\n",
    "\n",
    "# print(\"Пример uplift:\")\n",
    "# print(uplift_pred[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "67b02965-ccc8-4bfb-b4e9-af9ef430726b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "import causalml.inference.meta.tlearner as tlearner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8fac53e-fa96-495a-9299-1a5266efe4e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import sys\n",
    " \n",
    "if sys.argv:\n",
    "    sys.path.insert(0, str(Path('/Users/ogrobertino/UpliftModelingResearch/').resolve()))\n",
    "\n",
    "# %load_ext autoreload\n",
    "# %autoreload 2\n",
    "import pandas as pd\n",
    "from src.distributions_check import check_feature_distributions_by_stat_test, check_feature_distributions_by_model\n",
    "from src.datasets import sample_features, TorchDataset, NumpyDataset\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from causalml.inference.tree import UpliftTreeClassifier\n",
    "from causalml.inference.meta import BaseTLearner\n",
    "from sklearn.metrics import classification_report, roc_auc_score\n",
    "import causalml.inference.meta.tlearner as tlearner\n",
    "import lightgbm as lgb\n",
    "\n",
    "\n",
    "def generate_synthetic_data(n=5000):\n",
    "    np.random.seed(42)\n",
    "    X1 = np.random.normal(0, 1, n)\n",
    "    X2 = np.random.normal(0, 1, n)\n",
    "    X3 = np.random.normal(0, 1, n)\n",
    "\n",
    "    # Фичи\n",
    "    X = pd.DataFrame({\"X1\": X1, \"X2\": X2, \"X3\": X3})\n",
    "\n",
    "    # Третий признак влияет на вероятность записи лечения\n",
    "    treatment_effect = (X3 > 0).astype(int)\n",
    "    treatment = np.random.binomial(1, 0.5 + 0.2 * treatment_effect)\n",
    "\n",
    "    # Целевая метка\n",
    "    y = np.random.binomial(1, 0.3 + 0.4 * treatment * np.clip(X1 + X2, 0, 1))\n",
    "\n",
    "    return X, treatment, y\n",
    "\n",
    "# Генерация данных\n",
    "X, treatment, y = generate_synthetic_data()\n",
    "tmodel = tlearner.BaseTClassifier(\n",
    "    # learner=lgb.LGBMClassifier(max_depth=4),\n",
    "    control_learner=lgb.LGBMClassifier(max_depth=2),\n",
    "    treatment_learner=lgb.LGBMClassifier(max_depth=2),\n",
    "    control_name=0\n",
    ")\n",
    "tmodel.fit(X=X, treatment=treatment, y=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f46c2568-9659-433b-990f-5259a5d00b25",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmodel = tlearner.BaseTClassifier(\n",
    "    # learner=lgb.LGBMClassifier(max_depth=4),\n",
    "    control_learner=lgb.LGBMClassifier(),\n",
    "    treatment_learner=lgb.LGBMClassifier(),\n",
    "    control_name=0\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a46aa211-8c29-45ee-9532-b844a89d3725",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3500,), (3500,), (3500, 3))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "treatment_train.shape, y_train.shape, X_train.values.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3922dc97-7168-4e6e-bdff-12b368ed99bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ogrobertino/new_diploma/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 370, number of negative: 1010\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000535 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 765\n",
      "[LightGBM] [Info] Number of data points in the train set: 1380, number of used features: 3\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.268116 -> initscore=-1.004203\n",
      "[LightGBM] [Info] Start training from score -1.004203\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ogrobertino/new_diploma/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 909, number of negative: 1211\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000172 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 765\n",
      "[LightGBM] [Info] Number of data points in the train set: 2120, number of used features: 3\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.428774 -> initscore=-0.286857\n",
      "[LightGBM] [Info] Start training from score -0.286857\n"
     ]
    }
   ],
   "source": [
    "tmodel.fit(X=X_train.values, treatment=treatment_train, y=y_train)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
